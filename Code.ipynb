{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Necessary Imports\n",
    "\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd \n",
    "import os \n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Netclan20241017</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-ml-bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Netclan20241018</td>\n",
       "      <td>https://insights.blackcoffer.com/enhancing-fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Netclan20241019</td>\n",
       "      <td>https://insights.blackcoffer.com/roas-dashboar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Netclan20241020</td>\n",
       "      <td>https://insights.blackcoffer.com/efficient-pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Netclan20241021</td>\n",
       "      <td>https://insights.blackcoffer.com/development-o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                                URL\n",
       "0  Netclan20241017  https://insights.blackcoffer.com/ai-and-ml-bas...\n",
       "1  Netclan20241018  https://insights.blackcoffer.com/enhancing-fro...\n",
       "2  Netclan20241019  https://insights.blackcoffer.com/roas-dashboar...\n",
       "3  Netclan20241020  https://insights.blackcoffer.com/efficient-pro...\n",
       "4  Netclan20241021  https://insights.blackcoffer.com/development-o..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the Data \n",
    "\n",
    "data = pd.read_excel(r\"C:\\Users\\Admin\\OneDrive\\Desktop\\DATASETS\\Blackcoffer\\Input.xlsx\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Extraction\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    url = row['URL']\n",
    "    url_id = row['URL_ID']\n",
    "    \n",
    "    header = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\"}\n",
    "    try: \n",
    "        response = requests.get(url, headers=header)\n",
    "    except: \n",
    "        print(\"can't get response of {}\".format(url_id))\n",
    "    try:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    except:\n",
    "        print(\"can't get page of {}\".format(url_id))\n",
    "    try:\n",
    "        title= soup.find('h1').get_text()\n",
    "    except:\n",
    "        print(\"can't get title of {}\".format(url_id))\n",
    "        continue \n",
    "\n",
    "    article = \"\"\n",
    "    try: \n",
    "        for p in soup.find_all('p'):\n",
    "            article += p.get_text()\n",
    "    except:\n",
    "        print(\"can't get text of {}\". format(url_id))\n",
    "\n",
    "    file_name = r\"C:\\Users\\Admin\\OneDrive\\Desktop\\DATASETS\\Blackcoffer\\titletext\"  + str(url_id) + '.txt'\n",
    "    with open(file_name, 'w', encoding=\"utf-8\") as file : \n",
    "        file.write(title + '\\n' + article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracted Data and Other necessary Directories\n",
    "\n",
    "text_dir  = r\"C:\\Users\\Admin\\OneDrive\\Desktop\\DATASETS\\Blackcoffer\\TitleText\"\n",
    "stopwords_dir = r\"C:\\Users\\Admin\\OneDrive\\Desktop\\DATASETS\\Blackcoffer\\StopWords\"\n",
    "sentment_dir = r\"C:\\Users\\Admin\\OneDrive\\Desktop\\DATASETS\\Blackcoffer\\MasterDictionary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Analysis\n",
    "\n",
    "from nltk import word_tokenize \n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "stop_words = set()\n",
    "for files in os.listdir(stopwords_dir):\n",
    "    try : \n",
    "        with open(os.path.join(stopwords_dir,files), 'r', encoding=' ISO-8859-1', errors='ignore') as f:\n",
    "            stop_words.update(set(f.read().splitlines()))\n",
    "    except FileNotFoundError : \n",
    "         print(\"Warning:\", files, \"not found in\", stopwords_dir)\n",
    "          \n",
    "docs = []\n",
    "for text_file in os.listdir(text_dir):\n",
    "    with open(os.path.join(text_dir,text_file), 'r',encoding='ISO-8859-1',errors='ignore') as f :\n",
    "        text = f.read()\n",
    "        words = word_tokenize(f.read())\n",
    "        filtered_text = [word for word in words if word.lower() not in stop_words]\n",
    "        docs.append(filtered_text)\n",
    "\n",
    "pos=set()\n",
    "neg=set()\n",
    "\n",
    "for files in os.listdir(sentment_dir):\n",
    "    if files == 'positive.words.txt':\n",
    "        with open(os.path.join(sentment_dir,files),'r',encoding='ISO-8859-1', errors='ignore') as f :\n",
    "            pos.update(f.read().splitlines())\n",
    "    else: \n",
    "        with open(os.path.join(sentment_dir,files),'r', encoding = 'ISO-8859-1', errors='ignore') as f :\n",
    "            neg.update(f.read().splitlines())\n",
    "\n",
    "positive_words = []\n",
    "Negative_words = []\n",
    "positive_score = []\n",
    "negative_score = []\n",
    "polarity_score = []\n",
    "subjectivity_score = []\n",
    "\n",
    "\n",
    "for i in range(len(docs)):\n",
    "    positive_words.append([word for word in docs[i] if word.lower() in pos ])\n",
    "    Negative_words.append([word for word in docs[i] if word.lower()in neg ])\n",
    "    positive_score.append(len(positive_words[i]))\n",
    "    negative_score.append(len(Negative_words[i]))\n",
    "    polarity_score.append((positive_score[i]- negative_score[i])/ ((len(docs[i]))+ 0.000001))\n",
    "    subjectivity_score.append((positive_score[i] + negative_score[i]) / ((len(docs[i])) + 0.000001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sentence_length = []\n",
    "Percentage_of_Complex_words = []\n",
    "Fog_Index = []\n",
    "complex_word_count = []\n",
    "avg_syllable_word_count = []\n",
    "stopwords = set(stopwords.words('english'))\n",
    "def measure(file):\n",
    "    with open(os.path.join(text_dir, file),'r', encoding='ISO-8859-1', errors='ignore')as f:\n",
    "        text = f.read()\n",
    "        text = re.sub(r'[^\\w\\s.]','',text)\n",
    "        sentences = text.split('.')\n",
    "        num_sentences = len(sentences)\n",
    "        words = [word for word in text.split() if word.lower() not in stopwords]\n",
    "        num_words = len(words)\n",
    "        complex_words = []\n",
    "        for word in words : \n",
    "            vowels = 'aeiou'\n",
    "            syllable_count_word = sum( 1 for letter in word if letter.lower() in vowels)\n",
    "            if syllable_count_word > 2 :\n",
    "                complex_words.append(word)\n",
    "        syllable_count = 0 \n",
    "        syllable_words = []\n",
    "        for word in words :\n",
    "            if word.endswith('es'):\n",
    "                word = word[:-2]\n",
    "            elif word.endswith('ed'):\n",
    "                word = words[:-2] \n",
    "            vowels = 'aeiou'\n",
    "            syllable_count_word = sum( 1 for letter in word if letter.lower() in vowels)\n",
    "            if syllable_count_word >= 1 :\n",
    "                syllable_words.append(word)\n",
    "                syllable_count += syllable_count_word\n",
    "\n",
    "        avg_sentence_len = num_words / num_sentences\n",
    "        avg_syllable_word_count = syllable_count / len(syllable_words)\n",
    "        Percent_Complex_words = len(complex_words) / num_words\n",
    "        Fog_Index = 0.4 * (avg_sentence_len + Percent_Complex_words)\n",
    "\n",
    "        return avg_sentence_len, Percent_Complex_words, Fog_Index, len(complex_words), avg_syllable_word_count\n",
    "        \n",
    "\n",
    "for file in os.listdir(text_dir):\n",
    "    x,y,z,a,b = measure(file)\n",
    "    avg_sentence_length.append(x)\n",
    "    Percentage_of_Complex_words.append(y)\n",
    "    Fog_Index.append(z)\n",
    "    complex_word_count.append(a)\n",
    "    avg_syllable_word_count.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned_words(file):\n",
    "    with open(os.path.join(text_dir, file), 'r',encoding='ISO-8859-1',errors='ignore') as f:\n",
    "        text = f.read()\n",
    "        text= re.sub(r'[^\\w\\s]','',text)\n",
    "        words = [word for word in text.split() if word.lower() not in stopwords]\n",
    "        length = sum(len(word) for word in words)\n",
    "        average_word_length = length / len(words)\n",
    "    return len(words), average_word_length\n",
    "\n",
    "word_count = []\n",
    "average_word_length = []\n",
    "for file in os.listdir(text_dir):\n",
    "    x,y = cleaned_words(file)\n",
    "    word_count.append(x)\n",
    "    average_word_length.append(y)\n",
    "\n",
    "def count_personal_pronouns(file):\n",
    "    with open(os.path.join(text_dir, file), 'r',errors='ignore',encoding='ISO-8859-1') as f: \n",
    "        text = f.read()\n",
    "        personal_pronouns = ['I','we', 'my', 'ours','us']\n",
    "        count = 0 \n",
    "        for pronoun in personal_pronouns:\n",
    "            count += len(re.findall(r\"\\b\" + pronoun + r\"\\b\", text))\n",
    "    return count\n",
    "\n",
    "pp_count = []\n",
    "for file in os.listdir(text_dir):\n",
    "    x = count_personal_pronouns(file)\n",
    "    pp_count.append(x)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df = pd.read_excel(r\"C:\\Users\\Admin\\OneDrive\\Desktop\\DATASETS\\Blackcoffer\\Output Data Structure.xlsx\")\n",
    "len(output_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "\n",
    "for i, value in enumerate(positive_score):\n",
    "    output_df.iloc[:,2] = value\n",
    "\n",
    "for i, value in enumerate(negative_score):\n",
    "    output_df.iloc[:,3]= value\n",
    "\n",
    "for i, value in enumerate(polarity_score):\n",
    "    output_df.iloc[:,4] = value\n",
    "\n",
    "for i, value in enumerate(subjectivity_score):\n",
    "    output_df.iloc[:,5] = value\n",
    "\n",
    "for i, value in enumerate(avg_sentence_length):\n",
    "    output_df.iloc[:,6] = value\n",
    "\n",
    "for i, value in enumerate(Percentage_of_Complex_words):\n",
    "    output_df.iloc[:,7] = value\n",
    "\n",
    "for i, value in enumerate(Fog_Index):\n",
    "    output_df.iloc[:,8] = value\n",
    "\n",
    "for i, value in enumerate(avg_sentence_length):\n",
    "    output_df.iloc[:,9] = value\n",
    "\n",
    "for i, value in enumerate(complex_word_count):\n",
    "    output_df.iloc[:,10] = value\n",
    "\n",
    "for i, value in enumerate(word_count):\n",
    "    output_df.iloc[:,11] = value\n",
    "    \n",
    "for i, value in enumerate(avg_syllable_word_count):\n",
    "    output_df.iloc[:,12] = value\n",
    "    \n",
    "for i, value in enumerate(pp_count):\n",
    "    output_df.iloc[:,13] = value\n",
    "    \n",
    "for i, value in enumerate(average_word_length):\n",
    "    output_df.iloc[:,14] = value\n",
    "    \n",
    "    \n",
    "output_df.to_csv(r\"C:\\Users\\Admin\\OneDrive\\Desktop\\DATASETS\\Blackcoffer\\Output.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
